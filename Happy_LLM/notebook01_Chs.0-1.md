### Chapter 00
##### LLM发展
```
 LLM 发展
├── NLP 历史演进
│   ├── 符号主义阶段——>统计学习阶段——>深度学习阶段——>预训练模型阶段（PLM）——>大模型阶段（LLM）
│
├── PLM（上一阶段核心成果）
│   ├── 代表：GPT、BERT
│   ├── 架构：注意力机制
│   ├── 方法：预训练 + 微调
│   │   └─ 海量无监督文本 → 自监督预训练
│   ├── 优势：强大自然语言理解能力
│   └── 局限
│       ├── 依赖一定量有监督数据微调、生成任务性能不尽如人意、距通用人工智能仍有差距
│
├── LLM（PLM 的衍生成果 & 突破）
│   ├── 实现手段
│   │   ├── 大量扩大模型参数
│   │   ├── 扩大预训练数据规模
│   │   ├── 指令微调
│   │   └── 人类反馈强化学习（RLHF）
│   ├── 核心能力（相较 PLM 的突破）
│       ├── 涌现能力 + 强大上下文学习能力 + 指令理解能力 + 强大文本生成能力
|       └── 研究范式转变
│       ├── 减少对大量监督标注依赖
│       ├── 少量示例 → 媲美大规模微调性能
│       └── 直接、高效、准确响应用户指令
│
└── LLM 阶段现状与未来（2023 年至今)
    ├── 阶段性成果层出不穷
       ├── ChatGPT
       ├── GPT-4
       ├── 推理大模型（代表：DeepSeek-R1）

       └── 多模态大模型（代表：Qwen-VL）
```
---
### Chapter 01  NLP基本概念
NLP让计算机理解解释生成人类语言
##### 以下是几个NLP任务：
* 中文分词：将连续的中文文本切分成有意义的词汇序列，分词结果会影响后续词性标注、实体识别、句法分析等任务
* 字词切分：将词汇进一步分解为更小的单位，这样就可以通过已知字词单位生成理解新词。常用在合成词多的语言、预处理模型上
* 词性标注：为文本中的每个单词分配一个词性标签，让计算机更好理解文本含义
* 文本分类：把给定文本分配到一个或多个预定义的类别中。成功关键在于特征表示和分类算法，以及拥有高质量的训练数据。趋势使用神经网络文本分类可以捕捉文本的复杂模式和语意信息
* 实体识别：自动识别文本中具有特定意义的实体，并将它们分类为预定义的类别，如人名、地点、组织、日期、时间等。应用在信息提取、知识图谱构建、问答系统、内容推荐等领域
* 关系抽取：从文本中识别实体之间的语义关系
* 文本摘要：
  * 抽取式摘要：直接从原文选关键字拼接
  * 生成式摘要：选片段->重新组织改写->生成新内容  （Seq2Seq）
* 机器翻译：不仅涉及到词汇的直接转换，更重要的是要准确传达源语言文本的语义、风格和文化背景等。如基于神经网络的Seq2Seq模型、Transformer模型等，这些模型能够学习到源语言和目标语言之间的复杂映射关系
##### 文本表示
将文本数据数字化，涉及到将文本中的*语言单位*（如字、词、短语、句子等）以及它们之间的*关系*和*结构信息*转换为计算机能够理解和操作的形式，例如向量、矩阵或其他数据结构。还需要考虑计算效率和存储效率。
###### 文本表示的发展
1. 词向量：将文本（词、句、段落或文档）映射为高维向量空间中的点，每个维度对应一个特征项（词、词组等），向量元素值为该特征的权重（常用 TF、TF-IDF 等计算），从而把非结构化文本转化为可进行数学运算的形式
    * 优势：
        * 实现了文本的数学化表示，便于计算相似度、分类、聚类、信息检索等任务
        * 支持高效的向量运算和矩阵分解（如 SVD）优化
    * 缺陷：
        * 数据稀疏性与维数灾难：特征维度极高（往往数十万到数百万维），大部分元素为零，计算和存储成本巨大
        * 独立性假设：完全忽略词序、句法结构和上下文语义，导致语义表达能力严重不足
    * 优化方法：
        * 改进特征表示方法，如借助图方法、主题方法等进行关键词抽取
        * 改进和优化特征项权重的计算方法，可以在现有方法的基础上进行融合计算或提出新的计算方法
 2. 语言模型：是 NLP 领域中一种基于统计的语言模型，核心思想是基于马尔可夫假设，即一个词的出现概率仅依赖于它前面的N-1个词
    * 优势：实现简单、容易理解
    * 缺陷：但当N较大时，会出现数据稀疏性问题
 3. Word2Vec：是一种词嵌入技术。种基于神经网络NNLM的语言模型，旨在通过学习词与词之间的*上下文关系*来生成*词的密集向量表示*
    * 连续词袋模型CBOW(Continuous Bag of Words)：上下文对应词向量——>目标词向量
    * Skip-Gram：目标词向量——>上下文对应词向量
    * 优势：
         * 生成的是低维（通常几百维）的密集向量，有助于减少计算复杂度和存储需求
         * 捕捉到词与词之间的语义关系
         * 泛化到未见过的词
    * 缺陷：
         * 基于局部上下文的，无法捕捉到长距离的依赖关系，缺乏整体的词与词之间的关系
  4. ELMo：实现了一词多义、静态词向量到动态词向量的跨越式转变。能够捕捉到词汇的多义性和上下文信息，
     * 在大型语料库上训练语言模型，得到词向量模型 => 在做特定任务时, 从预训练网络中提取对应单词的词向量作为新特征补充到下游任务中
