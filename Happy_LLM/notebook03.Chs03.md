#### Seq2Seq模型
将一个输入序列转化为另一个输出序列。输入和输出的长度不一定相同

* 编码：首先将输入序列（如中文句子）转化为一个隐藏的向量或矩阵，代表输入的语义。
* 解码：然后使用这个向量或矩阵生成对应的输出序列（如英文句子）。

Transformer 是一种广泛应用于 Seq2Seq 任务的模型，最初就是为了机器翻译而设计的。通过这样的编码-解码过程，几乎所有 NLP 任务（如文本分类、词性标注等）都可以被看作 Seq2Seq 问题
![encoder_decoder](https://github.com/liu66-qing/NOTEBOOK/blob/main/encoder_decoder.png)

Transformer 由 Encoder 和 Decoder 组成，每一个 Encoder（Decoder）又由 6个 Encoder（Decoder）Layer 组成。输入源序列会进入 Encoder 进行编码，到 Encoder Layer 的最顶层再将编码结果输出给 Decoder Layer 的每一层，通过 Decoder 解码后就可以得到输出目标序列了。
---
#### 前馈神经网络
前馈神经网络（Feed Forward Neural Network，FNN），每一层的神经元都和上下两层的每一个神经元完全连接的网络结构。每一个 Encoder Layer 都包含一个上文讲的注意力机制和一个前馈神经网络。

前馈神经网络的实现
```
class MLP(nn.Module):
    '''前馈神经网络'''
    def __init__(self, dim: int, hidden_dim: int, dropout: float):
        super().__init__()
        # 定义第一层线性变换，从输入维度到隐藏维度
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        # 定义第二层线性变换，从隐藏维度到输入维度
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        # 定义dropout层，用于防止过拟合
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # 前向传播函数
        # 首先，输入x通过第一层线性变换和RELU激活函数
        # 最后，通过第二层线性变换和dropout层
        return self.dropout(self.w2(F.relu(self.w1(x))))
    
```

#### 层归一化
层归一化（Layer Norm） 是深度学习中一种经典的归一化技术，它与 批归一化（Batch Norm） 一起是神经网络中常见的归一化方法。其核心目标是通过归一化操作，使不同层的输入分布更加一致。

归一化的目的：

在深度神经网络中，每一层的输入来自上一层的输出。由于每层的参数更新会影响下一层的输入分布，因此随着网络层数增加，后层的输入分布差异会变得越来越大。这个现象被称为“内部协方差偏移”（Internal Covariate Shift）。如果这些输入分布差异较大，就会影响模型的训练效果，导致预测误差。因此，归一化操作被引入，以确保各层输入的分布更加稳定，从而提高训练的效果。

##### 批归一化（Batch Norm）：

批归一化是一种常见的归一化方法，其基本流程如下：

计算均值：对 mini-batch 中的每个样本，计算其在每个特征维度上的均值（μ）：


其中，
𝑍
𝑗
𝑖
Z
ji
	​

 是样本 
𝑖
i 在第 
𝑗
j 个特征维度上的值，
𝑚
m 是 mini-batch 的大小。

计算方差：然后计算每个特征维度上的方差（σ²）：

𝜎
𝑗
2
=
1
𝑚
∑
𝑖
=
1
𝑚
(
𝑍
𝑗
𝑖
−
𝜇
𝑗
)
2
σ
j
2
	​

=
m
1
	​

i=1
∑
m
	​

(Z
ji
	​

−μ
j
	​

)
2

归一化操作：对每个样本的特征值进行归一化，转换成标准正态分布：

𝑍
𝑗
′
=
𝑍
𝑗
−
𝜇
𝑗
𝜎
𝑗
2
+
𝜖
Z
j
′
	​

=
σ
j
2
	​

+ϵ
	​

Z
j
	​

−μ
j
	​

	​


其中，
𝜖
ϵ 是一个小常数，用于避免分母为0的情况。

批归一化虽然在许多网络中表现良好，但它也存在一些缺点：

小批量问题：当显存有限或 mini-batch 较小，计算的均值和方差可能无法很好地反映全局统计特性，导致性能下降。

RNN 中的局限性：在处理时间序列数据（如 RNN）时，不同时间步之间的分布差异较大，使用批归一化可能会导致归一化失去意义。

统计信息存储问题：批归一化需要在训练过程中保存每个 step 的均值和方差信息，而在测试阶段，尤其是处理变长句子时，训练时的统计信息可能不适用。

计算开销：批归一化在每个 step 中需要保存和计算 batch 统计量，增加了计算的时间和空间开销。

层归一化（Layer Norm）的优势：

层归一化通过对每个样本的所有特征维度计算均值和方差来进行归一化，从而克服了批归一化在小批量和时间序列任务中的问题。其主要优点是：

在每个样本内计算：层归一化在每个样本的所有特征上进行归一化，不依赖于批次大小。因此，无论批量大小如何变化，层归一化都能保持稳定的效果。

适用于 RNN 和序列数据：由于层归一化是在每个样本内部进行归一化，尤其适合处理变长序列或 RNN 任务，在这些任务中，批归一化常常会因样本差异性大而导致效果不佳。

层归一化的计算方式：

层归一化的计算过程和批归一化类似，只是统计的维度不同：

计算均值：对于每个样本，计算它所有特征的均值（μ）：

𝜇
=
1
𝐻
∑
ℎ
=
1
𝐻
𝑍
ℎ
μ=
H
1
	​

h=1
∑
H
	​

Z
h
	​


其中 
𝐻
H 是该样本的特征数，
𝑍
ℎ
Z
h
	​

 是该样本在第 
ℎ
h 个特征维度的值。

计算方差：计算每个样本所有特征的方差（σ²）：

𝜎
2
=
1
𝐻
∑
ℎ
=
1
𝐻
(
𝑍
ℎ
−
𝜇
)
2
σ
2
=
H
1
	​

h=1
∑
H
	​

(Z
h
	​

−μ)
2

归一化操作：然后对每个样本进行标准化处理：

𝑍
ℎ
′
=
𝑍
ℎ
−
𝜇
𝜎
2
+
𝜖
Z
h
′
	​

=
σ
2
+ϵ
	​

Z
h
	​

−μ
	​


其中，
𝜖
ϵ 用于避免分母为0。
